{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 自动求导机制通过有向无环图（directed acyclic graph ，DAG）实现\n",
    "- 在DAG中，记录数据（对应tensor.data）以及操作（对应tensor.grad_fn）\n",
    "- 操作在pytorch中统称为Function，如加法、减法、乘法、ReLU、conv、Pooling等，统统是Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None, inputs=None)\n",
    "- 用于对张量进行反向传播计算梯度，参数包括张量列表、梯度张量列表、是否保留计算图、是否创建计算图、梯度变量列表和输入变量列表\n",
    "- tensors (Sequence[Tensor] or Tensor) – 用于求导的张量\n",
    "- grad_tensors (Sequence[Tensor or None] or Tensor, optional) – 雅克比向量积中使用，详细作用请看代码演示\n",
    "- retain_graph (bool, optional) – 是否需要保留计算图。pytorch的机制是在方向传播结束时，计算图释放以节省内存。大家可以尝试连续使用loss.backward()，就会报错。如果需要多次求导，则在执行backward()时，retain_graph=True\n",
    "- create_graph (bool, optional) – 是否创建计算图，用于高阶求导\n",
    "- 输入（Sequence[Tensor]或 Tensor，可选）- 与之相关的梯度将累积到.grad 中,所有其他 Tensor 将被忽略;如果不提供，梯度将累积到用于计算 attr::tensors 的所有叶 Tensor 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.])\n",
      "tensor([10.])\n"
     ]
    }
   ],
   "source": [
    "#retain_grad参数的使用\n",
    "import torch\n",
    "\n",
    "w = torch.tensor([1.],requires_grad = True)\n",
    "x = torch.tensor([2.],requires_grad = True)\n",
    "\n",
    "a = torch.add(w,x)\n",
    "b = torch.add(w,1)\n",
    "y = torch.mul(a,b)\n",
    "\n",
    "y.backward(retain_graph = True)\n",
    "print(w.grad)\n",
    "# 由于 PyTorch 梯度是累加的\n",
    "# 第二次反向传播的结果会加到第一次的结果上\n",
    "y.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m y.backward(retain_graph = \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(w.grad)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(w.grad)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jupyter/lib/python3.13/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jupyter/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jupyter/lib/python3.13/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "#### retain_grad = False\n",
    "w = torch.tensor([1.],requires_grad = True)\n",
    "x = torch.tensor([2.],requires_grad = True)\n",
    "\n",
    "a = torch.add(w,x)\n",
    "b = torch.add(w,1)\n",
    "y = torch.mul(a,b)\n",
    "\n",
    "y.backward(retain_graph = False)\n",
    "print(w.grad)\n",
    "y.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 5.], grad_fn=<CatBackward0>)\n",
      "tensor([9.])\n"
     ]
    }
   ],
   "source": [
    "# grad_tensors的使用\n",
    "\n",
    "w = torch.tensor([1.],requires_grad = True)\n",
    "x = torch.tensor([2.],requires_grad = True)\n",
    "\n",
    "a = torch.add(w,x)\n",
    "b = torch.add(w,1)\n",
    "\n",
    "y0 = torch.mul(a,b)       # y0 = (x+w) * (w+1)    dy0/dw = 2w + x + 1\n",
    "y1 = torch.add(a,b)       # y1 = (x+w) + (w+1)    dy1/dw = 2\n",
    "\n",
    "loss = torch.cat([y0,y1],dim = 0)      # [y0, y1]\n",
    "print(loss)\n",
    "grad_tensors  = torch.tensor([1.,2.])\n",
    "\n",
    "# Tensor.backward中的 gradient 传入 torch.autograd.backward()中的grad_tensors\n",
    "loss.backward(gradient = grad_tensors)\n",
    "\n",
    "# w =  1* (dy0/dw)  +   2*(dy1/dw)\n",
    "# w =  1* (2w + x + 1)  +   2*(w)\n",
    "# w =  1* (5)  +   2*(2)\n",
    "# w =  9\n",
    "\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False)\n",
    "- 功能：计算outputs对inputs的导数\n",
    "- 主要参数：\n",
    "- outputs (sequence of Tensor) – 用于求导的张量，如loss\n",
    "- inputs (sequence of Tensor) – 所要计算导数的张量\n",
    "- grad_outputs (sequence of Tensor) – 雅克比向量积中使用\n",
    "- retain_graph (bool, optional) – 是否需要保留计算图。pytorch的机制是在方向传播结束时，计算图释放以节省内存。大家可以尝试连续使用loss.backward()，就会报错。如果需要多次求导，则在执行backward()时,retain_graph=True\n",
    "- create_graph (bool, optional) – 是否创建计算图，用于高阶求导\n",
    "- allow_unused (bool, optional) – 是否需要指示，计算梯度时未使用的张量是错误的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([6.], grad_fn=<MulBackward0>),)\n",
      "(tensor([2.]),)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3.],requires_grad=True)\n",
    "y = torch.pow(x,2)\n",
    "\n",
    "# 一阶导数\n",
    "grad_1 = torch.autograd.grad(y,x,create_graph = True)  # grad_1 = dy/dx = 2x = 2 * 3 = 6\n",
    "print(grad_1)\n",
    "\n",
    "#二阶导数\n",
    "grad_2  = torch.autograd.grad(grad_1[0],x)      # grad_2 = d(dy/dx)/dx = d(2x)/dx = 2\n",
    "print(grad_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.autograd.Function\n",
    "- 有的时候，想要实现自己的一些操作（op），如特殊的数学函数、pytorch的module中没有的网络层，那就需要自己写一个Function，在Function中定义好forward的计算公式、backward的计算公式，然后将这些op组合到模型中，模型就可以用autograd完成梯度求取\n",
    "- 这个概念还是很抽象，平时用得不多，但是自己想要自定义网络时，常常需要自己写op，那么它就很好用了，为了让大家掌握自定义op——Function的写法，特地从多处收集了四个案例，大家多运行代码体会Function如何写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.7183], grad_fn=<ExpBackward>)\n",
      "tensor([2.7183])\n"
     ]
    }
   ],
   "source": [
    "# eg1\n",
    "from torch.autograd import Function\n",
    "class Exp(Function):                    # 此层计算e^x\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):                # 模型前向\n",
    "        result = i.exp()\n",
    "        ctx.save_for_backward(result)   # 保存所需内容，以备backward时使用，所需的结果会被保存在saved_tensors元组中；此处仅能保存tensor类型变量，若其余类型变量（Int等），可直接赋予ctx作为成员变量，也可以达到保存效果\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):     # 模型梯度反传\n",
    "        result, = ctx.saved_tensors     # 取出forward中保存的result\n",
    "        return grad_output * result     # 计算梯度并返回\n",
    "\n",
    "# 尝试使用\n",
    "x = torch.tensor([1.], requires_grad=True)  # 需要设置tensor的requires_grad属性为True，才会进行梯度反传\n",
    "ret = Exp.apply(x)                          # 使用apply方法调用自定义autograd function\n",
    "print(ret)                                  # tensor([2.7183], grad_fn=<ExpBackward>)\n",
    "ret.backward()                              # 反传梯度\n",
    "print(x.grad)                               # tensor([2.7183])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.], grad_fn=<PowBackward0>)\n",
      "tensor([-0.4000])\n"
     ]
    }
   ],
   "source": [
    "# eg2\n",
    "class GradCoeff(Function):       \n",
    "       \n",
    "    @staticmethod\n",
    "    def forward(ctx, x, coeff):                 # 模型前向\n",
    "        ctx.coeff = coeff                       # 将coeff存为ctx的成员变量\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):             # 模型梯度反传\n",
    "        return ctx.coeff * grad_output, None    # backward的输出个数，应与forward的输入个数相同，此处coeff不需要梯度，因此返回None\n",
    "\n",
    "# 尝试使用\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "ret = GradCoeff.apply(x, -0.1)                  # 前向需要同时提供x及coeff，设置coeff为-0.1\n",
    "ret = ret ** 2                          \n",
    "print(ret)                                      # tensor([4.], grad_fn=<PowBackward0>)\n",
    "ret.backward()  \n",
    "print(x.grad)                                   # tensor([-0.4000])，梯度已乘以相应系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127.0\n"
     ]
    }
   ],
   "source": [
    "# eg3 \n",
    "import math\n",
    "\n",
    "class LegendrePolynomial3(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        y = 0.5 * (5 * x ** 3 - 3 * x)\n",
    "        ctx.save_for_backward(x)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        ret, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * ret ** 2 - 1)\n",
    "\n",
    "a, b, c, d = 1, 2, 1, 2 \n",
    "x = 1\n",
    "P3 = LegendrePolynomial3.apply\n",
    "y_pred = a + b * P3(c + d * x)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "梯度检查:  True\n",
      "反向传播前，weight.grad:  None\n",
      "反向传播后，weight.grad:  tensor([[[[1.1148, 1.0969, 1.1268],\n",
      "          [1.0943, 1.1037, 1.0788],\n",
      "          [1.0956, 1.1178, 1.1047]],\n",
      "\n",
      "         [[1.0829, 1.0741, 1.1091],\n",
      "          [1.1102, 1.1198, 1.1556],\n",
      "          [1.1514, 1.1903, 1.2224]],\n",
      "\n",
      "         [[1.1321, 1.1251, 1.1427],\n",
      "          [1.1727, 1.1364, 1.1209],\n",
      "          [1.1295, 1.0880, 1.0976]]],\n",
      "\n",
      "\n",
      "        [[[1.2814, 1.3066, 1.3202],\n",
      "          [1.3005, 1.3455, 1.2953],\n",
      "          [1.2799, 1.3291, 1.2988]],\n",
      "\n",
      "         [[1.2950, 1.3023, 1.3131],\n",
      "          [1.3244, 1.2974, 1.3382],\n",
      "          [1.3832, 1.4036, 1.4346]],\n",
      "\n",
      "         [[1.3653, 1.3588, 1.3755],\n",
      "          [1.3481, 1.3425, 1.3514],\n",
      "          [1.3506, 1.3239, 1.2739]]],\n",
      "\n",
      "\n",
      "        [[[1.0941, 1.1083, 1.1338],\n",
      "          [1.0983, 1.1176, 1.0827],\n",
      "          [1.0873, 1.1389, 1.0898]],\n",
      "\n",
      "         [[1.1026, 1.0919, 1.1319],\n",
      "          [1.1288, 1.1071, 1.1430],\n",
      "          [1.1660, 1.1792, 1.2043]],\n",
      "\n",
      "         [[1.1642, 1.1435, 1.1497],\n",
      "          [1.1875, 1.1279, 1.1282],\n",
      "          [1.1685, 1.1092, 1.1039]]],\n",
      "\n",
      "\n",
      "        [[[1.1313, 1.1353, 1.1537],\n",
      "          [1.1679, 1.1415, 1.1257],\n",
      "          [1.1157, 1.1771, 1.1337]],\n",
      "\n",
      "         [[1.1716, 1.1309, 1.1510],\n",
      "          [1.1770, 1.1428, 1.1843],\n",
      "          [1.2165, 1.2290, 1.2454]],\n",
      "\n",
      "         [[1.2087, 1.1808, 1.1745],\n",
      "          [1.1904, 1.2023, 1.1815],\n",
      "          [1.1899, 1.1260, 1.1464]]],\n",
      "\n",
      "\n",
      "        [[[1.3936, 1.3976, 1.4490],\n",
      "          [1.4213, 1.4250, 1.3856],\n",
      "          [1.3684, 1.4868, 1.4244]],\n",
      "\n",
      "         [[1.3955, 1.3706, 1.4362],\n",
      "          [1.4510, 1.4315, 1.4253],\n",
      "          [1.4820, 1.5179, 1.5287]],\n",
      "\n",
      "         [[1.4719, 1.4771, 1.4875],\n",
      "          [1.4796, 1.4722, 1.4617],\n",
      "          [1.4645, 1.4116, 1.3987]]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# eg4 手动实现2d卷积\n",
    "from torch.autograd.function import once_differentiable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def convolution_backward(grad_out, X, weight):\n",
    "    \"\"\"\n",
    "    将反向传播功能用函数包装起来，返回的参数个数与forward接收的参数个数保持一致，为2个\n",
    "    \"\"\"\n",
    "    grad_input = F.conv2d(X.transpose(0, 1), grad_out.transpose(0, 1)).transpose(0, 1)\n",
    "    grad_X = F.conv_transpose2d(grad_out, weight)\n",
    "    return grad_X, grad_input\n",
    "\n",
    "class MyConv2D(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, weight):\n",
    "        ctx.save_for_backward(X, weight)\n",
    "\n",
    "        # ============== step1: 函数功能实现 ==============\n",
    "        ret = F.conv2d(X, weight) \n",
    "        # ============== step1: 函数功能实现 ==============\n",
    "        return ret\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        X, weight = ctx.saved_tensors\n",
    "        return convolution_backward(grad_out, X, weight)\n",
    "      \n",
    "weight = torch.rand(5, 3, 3, 3, requires_grad=True, dtype=torch.double)\n",
    "X = torch.rand(10, 3, 7, 7, requires_grad=True, dtype=torch.double)\n",
    "# gradcheck 会检查你实现的自定义操作的前向传播 (forward) 和反向传播 (backward) 方法是否正确计算了梯度。\n",
    "# 如果返回 True，则表示梯度检查通过，即自定义操作的梯度计算与数值近似梯度之间的一致性在允许的误差范围内；\n",
    "# 如果返回 False，则说明存在不匹配，需要检查和修正自定义操作的反向传播逻辑。\n",
    "print(\"梯度检查: \", torch.autograd.gradcheck(MyConv2D.apply, (X, weight))) # gradcheck 功能请自行了解，通常写完Function会用它检查一下\n",
    "y = MyConv2D.apply(X, weight)\n",
    "label = torch.randn_like(y)\n",
    "loss = F.mse_loss(y, label)\n",
    "\n",
    "print(\"反向传播前，weight.grad: \", weight.grad)\n",
    "loss.backward()\n",
    "print(\"反向传播后，weight.grad: \", weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- autograd相关的知识点\n",
    "- 知识点一：梯度不会自动清零\n",
    "- 知识点二： 依赖于叶子结点的结点，requires_grad默认为True\n",
    "- 知识点三： 叶子结点不可执行in-place\n",
    "- 知识点四： detach 的作用\n",
    "- 知识点五： with torch.no_grad()的作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.])\n",
      "tensor([10.])\n",
      "tensor([15.])\n",
      "tensor([20.])\n"
     ]
    }
   ],
   "source": [
    "# 知识点1 : 梯度不会自动清零\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "for i in range(4):\n",
    "    a = torch.add(w, x)\n",
    "    b = torch.add(w, 1)\n",
    "    y = torch.mul(a, b)\n",
    "    y.backward()\n",
    "    print(w.grad)           # 梯度不会自动清零，会累加,通常采用optimizer.zero_grad()清零梯度\n",
    "\n",
    "# w.grad.zero_()  # 清零梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True\n",
      "False False False\n"
     ]
    }
   ],
   "source": [
    "# 知识点2 ： 依赖于叶子结点的结点，requires_grad默认为True\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "a = torch.add(w, x)\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "print(a.requires_grad, b.requires_grad, y.requires_grad)  # True True True\n",
    "print(a.is_leaf, b.is_leaf, y.is_leaf)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129936892929968 tensor([1.])\n",
      "129936893835488 tensor([2.])\n",
      "129936893835488 tensor([3.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m b = torch.add(w, \u001b[32m1\u001b[39m)\n\u001b[32m     18\u001b[39m y = torch.mul(a, b)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m y.backward()\n",
      "\u001b[31mRuntimeError\u001b[39m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "# 知识点3 ： 叶子张量不可以执行in-place操作\n",
    "# 叶子结点不可执行in-place，因为计算图的backward过程都依赖于叶子结点的计算，\n",
    "# 可以回顾计算图当中的例子，所有的偏微分计算所需要用到的数据都是基于w和x（叶子结点），因此叶子结点不允许in-place操作\n",
    "a = torch.ones((1,))\n",
    "print(id(a),a)\n",
    "\n",
    "a = a + torch.ones((1,))\n",
    "print(id(a),a)\n",
    "\n",
    "a += torch.ones((1,))\n",
    "print(id(a),a)\n",
    "\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "a = torch.add(w, x)\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "w.add_(1)\n",
    "\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([999.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 知识点4 ： detach的作用\n",
    "# 通过以上知识，我们知道计算图中的张量是不能随便修改的，否则会造成计算图的backward计算错误，\n",
    "# 那有没有其他方法能修改呢？当然有，那就是detach()\n",
    "\n",
    "# detach的作用是：从计算图中剥离出“数据”，并以一个新张量的形式返回，并且新张量与旧张量共享数据，简单的可理解为做了一个别名。 \n",
    "# 请看下例的w，detach后对w_detach修改数据，w同步地被改为了999\n",
    "\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "a = torch.add(w, x)\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "w_detach = w.detach()\n",
    "w_detach.data[0] = 999\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 知识点5 ：with torch.no_grad()的作用\n",
    "# autograd自动构建计算图过程中会保存一系列中间变量，以便于backward的计算，这就必然需要花费额外的内存和时间\n",
    "# 而并不是所有情况下都需要backward，例如推理的时候，因此可以采用上下文管理器——torch.no_grad()来管理上下文，\n",
    "# 让pytorch不记录相应的变量，以加快速度和节省空间"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
