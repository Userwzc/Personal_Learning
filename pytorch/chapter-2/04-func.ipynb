{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.cat(tensors, dim=0, *, out=None) → Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5127, -1.1283,  0.7264],\n",
      "        [ 1.7375, -0.5123,  1.6377]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(2,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3990,  0.7104,  0.5715],\n",
       "        [-0.0114,  0.1978,  1.9778],\n",
       "        [-0.3990,  0.7104,  0.5715],\n",
       "        [-0.0114,  0.1978,  1.9778],\n",
       "        [-0.3990,  0.7104,  0.5715],\n",
       "        [-0.0114,  0.1978,  1.9778]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x,x,x),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3990,  0.7104,  0.5715, -0.3990,  0.7104,  0.5715, -0.3990,  0.7104,\n",
       "          0.5715],\n",
       "        [-0.0114,  0.1978,  1.9778, -0.0114,  0.1978,  1.9778, -0.0114,  0.1978,\n",
       "          1.9778]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x,x,x),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.split(tensor, split_size_or_sections, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7],\n",
      "        [8, 9]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(10).reshape(5,2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1],\n",
       "         [2, 3]]),\n",
       " tensor([[4, 5],\n",
       "         [6, 7]]),\n",
       " tensor([[8, 9]]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果 split_size_or_sections 是整数类型，则 tensor 将被拆分为大小相等的块（如果可能）。\n",
    "# 如果沿给定维度 dim 的张量大小不能被 split_size 整除，则最后一个块将更小。\n",
    "torch.split(a,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1]]),\n",
       " tensor([[2, 3],\n",
       "         [4, 5],\n",
       "         [6, 7],\n",
       "         [8, 9]]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果 split_size_or_sections 是一个列表，\n",
    "# 那么 tensor 将根据 split_size_or_sections 被分成 len（split_size_or_sections） 个大小为 dim 的块。\n",
    "torch.split(a,[1,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.chunk(input: Tensor, chunks: int, dim: int = 0) → Tuple[Tensor, ...]\n",
    "- 尝试将张量拆分为指定数量的块。每个块都是输入张量的视图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1]),\n",
       " tensor([2, 3]),\n",
       " tensor([4, 5]),\n",
       " tensor([6, 7]),\n",
       " tensor([8, 9]),\n",
       " tensor([10]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(11).chunk(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1]),\n",
       " tensor([2, 3]),\n",
       " tensor([4, 5]),\n",
       " tensor([6, 7]),\n",
       " tensor([8, 9]),\n",
       " tensor([10, 11]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(12).chunk(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2]),\n",
       " tensor([3, 4, 5]),\n",
       " tensor([6, 7, 8]),\n",
       " tensor([ 9, 10, 11]),\n",
       " tensor([12]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(13).chunk(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.tensor_split(input, indices_or_sections, dim=0) → List of Tensors\n",
    "- 将一个张量拆分为多个子张量，所有这些子张量都是输入视图，根据 indices_or_sections 指定的索引或截面数沿维度 dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(8)\n",
    "torch.tensor_split(x,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2]), tensor([3, 4]), tensor([5, 6]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(7)\n",
    "torch.tensor_split(x,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0]), tensor([1, 2, 3, 4, 5]), tensor([6]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor_split(x,(1,6)) # [:1],[1:6],[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6],\n",
       "        [ 7,  8,  9, 10, 11, 12, 13]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(14).reshape(2,7)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2],\n",
       "         [7, 8, 9]]),\n",
       " tensor([[ 3,  4],\n",
       "         [10, 11]]),\n",
       " tensor([[ 5,  6],\n",
       "         [12, 13]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor_split(x,3,dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [7]]),\n",
       " tensor([[ 1,  2,  3,  4,  5],\n",
       "         [ 8,  9, 10, 11, 12]]),\n",
       " tensor([[ 6],\n",
       "         [13]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor_split(x,(1,6),dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.stack(tensors, dim=0, *, out=None) → Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3712, -0.6194,  1.6953],\n",
      "        [-2.0196, -0.0571, -0.6183]]) torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,3)\n",
    "print(x,x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.3712, -0.6194,  1.6953],\n",
      "         [-2.0196, -0.0571, -0.6183]],\n",
      "\n",
      "        [[ 1.3712, -0.6194,  1.6953],\n",
      "         [-2.0196, -0.0571, -0.6183]]]) torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.stack((x,x)),torch.stack((x,x)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.3712, -0.6194,  1.6953],\n",
      "         [ 1.3712, -0.6194,  1.6953]],\n",
      "\n",
      "        [[-2.0196, -0.0571, -0.6183],\n",
      "         [-2.0196, -0.0571, -0.6183]]]) torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.stack((x,x),dim =1),torch.stack((x,x),dim =1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.3712,  1.3712],\n",
      "         [-0.6194, -0.6194],\n",
      "         [ 1.6953,  1.6953]],\n",
      "\n",
      "        [[-2.0196, -2.0196],\n",
      "         [-0.0571, -0.0571],\n",
      "         [-0.6183, -0.6183]]]) torch.Size([2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(torch.stack((x,x),dim = 2),torch.stack((x,x),dim = 2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.3712,  1.3712],\n",
      "         [-0.6194, -0.6194],\n",
      "         [ 1.6953,  1.6953]],\n",
      "\n",
      "        [[-2.0196, -2.0196],\n",
      "         [-0.0571, -0.0571],\n",
      "         [-0.6183, -0.6183]]]) torch.Size([2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(torch.stack((x,x),dim = -1),torch.stack((x,x),dim = -1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.3712,  1.3712,  1.3712],\n",
      "         [-0.6194, -0.6194, -0.6194],\n",
      "         [ 1.6953,  1.6953,  1.6953]],\n",
      "\n",
      "        [[-2.0196, -2.0196, -2.0196],\n",
      "         [-0.0571, -0.0571, -0.0571],\n",
      "         [-0.6183, -0.6183, -0.6183]]]) torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.stack((x,x,x),dim = 2),torch.stack((x,x,x),dim = 2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.conj(input) → Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "tensor([-1.-1.j, -2.-2.j,  3.+3.j])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-1 + 1j, -2 + 2j,3 - 3j])\n",
    "print(x.is_conj())\n",
    "y =torch.conj(x)\n",
    "print(y)\n",
    "print(y.is_conj())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.dsplit(input, indices_or_sections) → List of Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.]],\n",
       "\n",
       "        [[ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(16.0).reshape(2,2,4)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.,  1.],\n",
       "          [ 4.,  5.]],\n",
       " \n",
       "         [[ 8.,  9.],\n",
       "          [12., 13.]]]),\n",
       " tensor([[[ 2.,  3.],\n",
       "          [ 6.,  7.]],\n",
       " \n",
       "         [[10., 11.],\n",
       "          [14., 15.]]]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这等效于调用 torch.tensor_split（input， indices_or_sections， dim=2）（split dimension 为 2），\n",
    "# 不同之处在于，如果 indices_or_sections 是整数，则必须均匀划分 split 维度，否则将引发运行时错误\n",
    "torch.dsplit(t,2)\n",
    "# torch.tensor_split(t,2,dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.,  1.,  2.],\n",
       "          [ 4.,  5.,  6.]],\n",
       " \n",
       "         [[ 8.,  9., 10.],\n",
       "          [12., 13., 14.]]]),\n",
       " tensor([[[ 3.],\n",
       "          [ 7.]],\n",
       " \n",
       "         [[11.],\n",
       "          [15.]]]),\n",
       " tensor([], size=(2, 2, 0)))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dsplit(t,[3,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.column_stack(tensors, *, out=None) → Tensor\n",
    "- 等效于 torch.hstack（tensors），不同之处在于张量中的每个零维或一维张量 t 在水平堆叠之前首先重塑为 （t.numel（）， 1） 列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4],\n",
       "        [2, 5],\n",
       "        [3, 6]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([4,5,6])\n",
    "torch.column_stack((a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 0, 1],\n",
       "        [1, 2, 3, 2, 3],\n",
       "        [2, 4, 5, 4, 5],\n",
       "        [3, 6, 7, 6, 7],\n",
       "        [4, 8, 9, 8, 9]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(5)\n",
    "b = torch.arange(10).reshape(5,2)\n",
    "torch.column_stack((a,b,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.dstack(tensors, *, out=None) → Tensor\n",
    "- 按顺序纵向堆叠张量（沿第三个轴）。\n",
    "- 这相当于在 1-D 和 2-D 张量被 torch.atleast_3d（） 重塑后沿第三轴的串联。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 4],\n",
      "         [2, 5],\n",
      "         [3, 6]]]) torch.Size([1, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([4,5,6])\n",
    "print(torch.dstack((a,b)),torch.dstack((a,b)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 4]],\n",
      "\n",
      "        [[2, 5]],\n",
      "\n",
      "        [[3, 6]]]) torch.Size([3, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1],[2],[3]])\n",
    "b = torch.tensor([[4],[5],[6]])\n",
    "print(torch.dstack((a,b)),torch.dstack((a,b)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.atleast_3d(*tensors)\n",
    "- 返回每个输入张量的 3 维视图，维度为零。具有三个或更多维度的输入张量按原样返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(0.5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5000]]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.atleast_3d(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.arange(4).view(2,2)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0],\n",
       "         [1]],\n",
       "\n",
       "        [[2],\n",
       "         [3]]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.atleast_3d(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(1).view(1,1,1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.atleast_3d(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.5000]]]), tensor([[[1.]]]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(0.5)\n",
    "y = torch.tensor(1.0)\n",
    "torch.atleast_3d((x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.gather(input, dim, index, *, sparse_grad=False, out=None) → Tensor\n",
    "- 可以将这个过程理解为：索引张量告诉函数\"从每行的哪个列位置提取元素\"，然后将提取的元素放置在对应的输出位置上\n",
    "- out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0\n",
    "- out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1\n",
    "- out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [4, 3]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[1,2],[3,4]])\n",
    "torch.gather(t,1,torch.tensor([[0,0],[1,0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.hsplit(input, indices_or_sections) → List of Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(16.0).reshape(4,4)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.],\n",
       "         [ 4.,  5.],\n",
       "         [ 8.,  9.],\n",
       "         [12., 13.]]),\n",
       " tensor([[ 2.,  3.],\n",
       "         [ 6.,  7.],\n",
       "         [10., 11.],\n",
       "         [14., 15.]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.hsplit(t,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.],\n",
       "         [ 4.,  5.,  6.],\n",
       "         [ 8.,  9., 10.],\n",
       "         [12., 13., 14.]]),\n",
       " tensor([[ 3.],\n",
       "         [ 7.],\n",
       "         [11.],\n",
       "         [15.]]),\n",
       " tensor([], size=(4, 0)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.hsplit(t,[3,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.hstack(tensors, *, out=None) → Tensor\n",
    "- This is equivalent to concatenation along the first axis for 1-D tensors, and along the second axis for all other tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([4,5,6])\n",
    "torch.hstack((a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4],\n",
       "        [2, 5],\n",
       "        [3, 6]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1],[2],[3]])\n",
    "b = torch.tensor([[4],[5],[6]])\n",
    "torch.hstack((a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.index_add(input: Tensor, dim: int, index: Tensor, source: Tensor, *, alpha: Union[Number, _complex] = 1, out: Optional[Tensor]) → Tensor\n",
    "- Tensor.index_add_(dim, index, source, *, alpha=1) → Tensor\n",
    "- self[index[i], :, :] += alpha * src[i, :, :]  # if dim == 0\n",
    "- self[:, index[i], :] += alpha * src[:, i, :]  # if dim == 1\n",
    "- self[:, :, index[i]] += alpha * src[:, :, i]  # if dim == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  3.,  4.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 8.,  9., 10.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 5.,  6.,  7.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(5,3)\n",
    "t = torch.tensor([[1,2,3],[4,5,6],[7,8,9]],dtype=torch.float32)\n",
    "index = torch.tensor([0,4,2])\n",
    "x.index_add(0,index,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., -1., -2.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [-6., -7., -8.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [-3., -4., -5.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.index_add(0,index,t,alpha=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.index_copy(input: Tensor, dim: int, index: Tensor, source: Tensor, *, out: Optional[Tensor]) → Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个目标张量\n",
    "input = torch.zeros(3, 5)\n",
    "# 创建一个源张量，包含要复制的数据\n",
    "source = torch.ones(2, 5)\n",
    "# 指定索引位置\n",
    "index = torch.tensor([0, 2])\n",
    "\n",
    "# 在dim=0维度上，将source的数据复制到input的第0行和第2行\n",
    "result = torch.index_copy(input, 0, index, source)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.index_reduce(input: Tensor, dim: int, index: Tensor, source: Tensor, reduce: str, *, include_self: bool = True, out: Optional[Tensor]) → Tensor\n",
    "- For a 3-D tensor with reduce=\"prod\" and include_self=True the output is given as:   \n",
    "self[index[i], :, :] *= src[i, :, :]  # if dim == 0   \n",
    "self[:, index[i], :] *= src[:, i, :]  # if dim == 1   \n",
    "self[:, :, index[i]] *= src[:, :, i]  # if dim == 2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40691/1925619273.py:4: UserWarning: index_reduce() is in beta and the API may change at any time. (Triggered internally at /pytorch/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1218.)\n",
      "  x.index_reduce(0,index,t,'prod')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[20., 44., 72.],\n",
       "        [ 2.,  2.,  2.],\n",
       "        [14., 16., 18.],\n",
       "        [ 2.,  2.,  2.],\n",
       "        [ 8., 10., 12.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.empty(5,3).fill_(2)\n",
    "t = torch.tensor([[1,2,3],[4,5,6],[7,8,9],[10,11,12]],dtype=torch.float32)\n",
    "index = torch.tensor([0,4,2,0])\n",
    "x.index_reduce(0,index,t,'prod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 22., 36.],\n",
       "        [ 2.,  2.,  2.],\n",
       "        [ 7.,  8.,  9.],\n",
       "        [ 2.,  2.,  2.],\n",
       "        [ 4.,  5.,  6.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.index_reduce(0,index,t,'prod',include_self=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.index_select(input, dim, index, *, out=None) → Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0257, -1.1746, -0.2725, -0.0853],\n",
       "        [ 0.3084,  2.4357, -0.2500,  1.4339],\n",
       "        [-0.1916,  0.9298, -0.2361,  1.4188]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3,4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0257, -1.1746, -0.2725, -0.0853],\n",
       "        [-0.1916,  0.9298, -0.2361,  1.4188]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.tensor([0,2])\n",
    "torch.index_select(x,0,indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0257, -0.2725],\n",
       "        [ 0.3084, -0.2500],\n",
       "        [-0.1916, -0.2361]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.index_select(x,1,indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.masked_select(input, mask, *, out=None) → Tensor\n",
    "- 返回一个新的 1-D 张量，该张量根据布尔掩码 mask 索引 input 张量，其中布尔掩码 mask 是一个 BoolTensor\n",
    "- The shapes of the mask tensor and the input tensor don’t need to match, but they must be **broadcastable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9909, -1.6578, -0.7618, -0.4770],\n",
       "        [ 1.0316, -1.6346, -0.8811,  0.4734],\n",
       "        [ 1.9939, -0.2066, -0.3350,  0.7763]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3,4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [ True, False, False, False],\n",
       "        [ True, False, False,  True]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = x.ge(0.5)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0316, 1.9939, 0.7763])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.masked_select(x,mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.movedim(input, source, destination) → Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9365],\n",
       "         [ 0.6670]],\n",
       "\n",
       "        [[-1.2468],\n",
       "         [ 0.3878]],\n",
       "\n",
       "        [[-1.2740],\n",
       "         [-0.5295]]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "t = torch.randn(3,2,1)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.movedim(t,1,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9365],\n",
       "         [-1.2468],\n",
       "         [-1.2740]],\n",
       "\n",
       "        [[ 0.6670],\n",
       "         [ 0.3878],\n",
       "         [-0.5295]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.movedim(t,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.movedim(t,(1,2),(0,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9365, -1.2468, -1.2740]],\n",
       "\n",
       "        [[ 0.6670,  0.3878, -0.5295]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.movedim(t,(1,2),(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始张量:\n",
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "\n",
      "变换后张量:\n",
      "tensor([[[ 0, 12],\n",
      "         [ 1, 13],\n",
      "         [ 2, 14],\n",
      "         [ 3, 15]],\n",
      "\n",
      "        [[ 4, 16],\n",
      "         [ 5, 17],\n",
      "         [ 6, 18],\n",
      "         [ 7, 19]],\n",
      "\n",
      "        [[ 8, 20],\n",
      "         [ 9, 21],\n",
      "         [10, 22],\n",
      "         [11, 23]]])\n",
      "\n",
      "原始元素 t[1,2,3] = 23\n",
      "变换后位置 result[2,3,1] = 23\n"
     ]
    }
   ],
   "source": [
    "# 创建一个形状为 (2, 3, 4) 的张量\n",
    "t = torch.arange(24).reshape(2, 3, 4)\n",
    "print(\"原始张量:\")\n",
    "print(t)\n",
    "\n",
    "# 使用 movedim\n",
    "result = torch.movedim(t, (1, 2), (0, 1))\n",
    "print(\"\\n变换后张量:\")\n",
    "print(result)\n",
    "\n",
    "# 验证特定元素位置\n",
    "i, j, k = 1, 2, 3  # 原始位置\n",
    "print(f\"\\n原始元素 t[{i},{j},{k}] = {t[i,j,k]}\")\n",
    "print(f\"变换后位置 result[{j},{k},{i}] = {result[j,k,i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.moveaxis(input, source, destination) → Tensor   \n",
    "Alias for torch.movedim()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.narrow(input, dim, start, length) → Tensor    \n",
    "返回一个新张量，它是 input 张量的缩小版本。维度 dim 从 start 到 start + length 输入。返回的张量和 input 张量共享相同的底层存储。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "torch.narrow(x,0,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3],\n",
       "        [6],\n",
       "        [9]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.narrow(x,-1,torch.tensor(-1),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.narrow_copy(input, dim, start, length, *, out=None) → Tensor\n",
    "与 Tensor.narrow() 相同，但这个函数返回一个副本而不是共享存储。这主要用于稀疏张量，因为稀疏张量没有共享存储的 narrow 方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "torch.narrow_copy(x,0,0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3],\n",
       "        [5, 6],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.narrow_copy(x,1,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 0],\n",
       "                       [0, 1]]),\n",
       "       values=tensor([[[0, 1],\n",
       "                       [2, 3]],\n",
       "\n",
       "                      [[4, 5],\n",
       "                       [6, 7]]]),\n",
       "       size=(1, 2, 2, 2), nnz=2, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = torch.arange(16).reshape(2,2,2,2).to_sparse(2)\n",
    "torch.narrow_copy(s,0,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.nonzero(input, *, out=None, as_tuple=False) → LongTensor or tuple of LongTensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [4]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(torch.tensor([1,1,1,0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [1, 1],\n",
       "        [2, 2],\n",
       "        [3, 3]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n",
    "                            [0.0, 0.4, 0.0, 0.0],\n",
    "                            [0.0, 0.0, 0.2, 0.0],\n",
    "                            [0.0, 0.0, 0.0, -0.4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 4]),)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(torch.tensor([1,1,1,0,1]),as_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0]),)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(torch.tensor(5),as_tuple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.permute(input, dims) → Tensor    \n",
    "  返回原始张量 input 的维度置换视图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(2,3,5)\n",
    "x.size()\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.permute(x,(2,0,1)).size()\n",
    "# torch.permute(x,(2,0,1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.reshape(input, shape) → Tensor    \n",
    "  返回一个与 input 具有相同数据和元素数量的张量，但具有指定的形状。当可能时，返回的张量将是 input 的视图。否则，它将是一个副本。连续输入和具有兼容步长的输入可以在不复制的情况下进行重塑，但您不应依赖于复制与查看的行为\n",
    "\n",
    "  查看 torch.Tensor.view() 了解何时可以返回视图\n",
    "\n",
    "  单维可以是 -1，在这种情况下，它将从剩余维度和 input 中的元素数量推断出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(4,)\n",
    "torch.reshape(a,(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor([[0,1],[2,3]])\n",
    "torch.reshape(b,(-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.row_stack(tensors, *, out=None) → Tensor    \n",
    "  按行堆叠张量,即第一个维度上增加，等同于torch.vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([4,5,6])\n",
    "torch.vstack((a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1],[2],[3]])\n",
    "b = torch.tensor([[4],[5],[6]])\n",
    "torch.vstack((a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.scatter(input, dim, index, src) → Tensor\n",
    "- torch.Tensor.scatter_的原地版本    \n",
    "- Tensor.scatter_(dim, index, src, *, reduce=None) → Tensor  \n",
    "- self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0     \n",
    "    \n",
    "- self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1    \n",
    "    \n",
    "- self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2\n",
    "    \n",
    "这是 gather() 中描述的方法的逆操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.arange(1,11).reshape((2,5))\n",
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 4, 0],\n",
       "        [0, 2, 0, 0, 0],\n",
       "        [0, 0, 3, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = torch.tensor([[0,1,2,0]])\n",
    "torch.zeros(3,5,dtype = src.dtype).scatter_(0,index,src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123803734258416, 123803734258416)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = torch.tensor([[0,1,2],[0,1,4]])\n",
    "out = torch.zeros(3,5,dtype = src.dtype)\n",
    "out_1= out.scatter_(1,index,src)\n",
    "id(out),id(out_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 2.0000, 2.4600, 2.0000],\n",
       "        [2.0000, 2.0000, 2.0000, 2.4600]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full((2,4),2.).scatter_(1,torch.tensor([[2],[3]]),1.23,reduce='multiply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0000, 2.0000, 3.2300, 2.0000],\n",
       "        [2.0000, 2.0000, 2.0000, 3.2300]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full((2,4),2.).scatter_(1,torch.tensor([[2],[3]]),1.23,reduce = 'add')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.scatter_reduce(input, dim, index, src, reduce, *, include_self=True) → Tensor    \n",
    "  Out-of-place version of torch.Tensor.scatter_reduce_() \n",
    "\n",
    "- Tensor.scatter_reduce_(dim, index, src, reduce, *, include_self=True) → Tensor     \n",
    "\n",
    "- For a 3-D tensor with reduce=\"sum\" and include_self=True the output is given as:    \n",
    "- self[index[i][j][k]][j][k] += src[i][j][k]  # if dim == 0\n",
    "- self[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1\n",
    "- self[i][j][index[i][j][k]] += src[i][j][k]  # if dim == 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5., 14.,  8.,  4.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "src = torch.tensor([1.,2.,3.,4.,5.,6.])\n",
    "index = torch.tensor([0,1,0,1,2,1])\n",
    "input = torch.tensor([1.,2.,3.,4.])\n",
    "input.scatter_reduce_(0,index,src,reduce=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4., 12.,  5.,  4.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.scatter_reduce_(0,index,src,reduce=\"sum\",include_self=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5., 6., 5., 2.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input2 = torch.tensor([5.,4.,3.,2.])\n",
    "input2.scatter_reduce(0,index,src,reduce=\"amax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 6., 5., 2.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input2.scatter_reduce(0,index,src,reduce=\"amax\",include_self=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.split(tensor, split_size_or_sections, dim=0)\n",
    "- 将张量分割成块。每个块都是原始张量的视图\n",
    "- 如果 split_size_or_sections 是整数类型，则 tensor 将被分割成大小相等的块（如果可能的话）。如果沿着给定维度 dim 的张量大小不能被 split_size 整除，最后一个块将更小\n",
    "- 如果 split_size_or_sections 是一个列表，则 tensor 将根据 split_size_or_sections 分割成 len(split_size_or_sections) 块，块的大小为 dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5],\n",
       "        [6, 7],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.arange(10).reshape(5,2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1],\n",
       "         [2, 3]]),\n",
       " tensor([[4, 5],\n",
       "         [6, 7]]),\n",
       " tensor([[8, 9]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(a,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1]]),\n",
       " tensor([[2, 3],\n",
       "         [4, 5],\n",
       "         [6, 7],\n",
       "         [8, 9]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.split(a,[1,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.squeeze(input: Tensor, dim: Optional[Union[int, List[int]]]) → Tensor\n",
    "- 返回一个移除了所有指定维度的 input ，大小为 1 的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 2, 1, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(2,1,2,1,2)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.squeeze(x)\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 2, 1, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.squeeze(x,0)\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 1, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.squeeze(x,1)\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.squeeze(x,(1,2,3))\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.stack(tensors, dim=0, *, out=None) → Tensor\n",
    "- 连接一系列张量沿新维度\n",
    "- 所有张量的大小必须相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4267, -0.7573,  1.2053],\n",
       "        [-0.4606,  0.9043,  1.0584]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack((x,x)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4267, -0.7573,  1.2053],\n",
       "         [-0.4267, -0.7573,  1.2053]],\n",
       "\n",
       "        [[-0.4606,  0.9043,  1.0584],\n",
       "         [-0.4606,  0.9043,  1.0584]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack((x,x),dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4267, -0.4267],\n",
       "         [-0.7573, -0.7573],\n",
       "         [ 1.2053,  1.2053]],\n",
       "\n",
       "        [[-0.4606, -0.4606],\n",
       "         [ 0.9043,  0.9043],\n",
       "         [ 1.0584,  1.0584]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack((x,x),dim = 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4267, -0.4267],\n",
       "         [-0.7573, -0.7573],\n",
       "         [ 1.2053,  1.2053]],\n",
       "\n",
       "        [[-0.4606, -0.4606],\n",
       "         [ 0.9043,  0.9043],\n",
       "         [ 1.0584,  1.0584]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack((x,x),dim = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.swapaxes(input, axis0, axis1) → Tensor\n",
    "- 等价于torch.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1],\n",
       "         [2, 3]],\n",
       "\n",
       "        [[4, 5],\n",
       "         [6, 7]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1],\n",
       "         [4, 5]],\n",
       "\n",
       "        [[2, 3],\n",
       "         [6, 7]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.swapaxes(x,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 4],\n",
       "         [2, 6]],\n",
       "\n",
       "        [[1, 5],\n",
       "         [3, 7]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.swapaxes(x,0,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.swapdims(input, dim0, dim1) → Tensor\n",
    "- 等价于torch.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1],\n",
       "         [2, 3]],\n",
       "\n",
       "        [[4, 5],\n",
       "         [6, 7]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])\n",
    "x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1],\n",
       "         [4, 5]],\n",
       "\n",
       "        [[2, 3],\n",
       "         [6, 7]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.swapdims(x,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 4],\n",
       "         [2, 6]],\n",
       "\n",
       "        [[1, 5],\n",
       "         [3, 7]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.swapdims(x,0,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.t(input) → Tensor         转置\n",
    "- 0-D 和 1-D 张量按原样返回,当输入是 2-D 张量时，这相当于 transpose(input, 0, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1906)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(())\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1906)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.t(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2924, -0.5149,  0.5470])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2924, -0.5149,  0.5470])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.t(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6929, 0.3914, 0.9603],\n",
       "        [0.5505, 0.3372, 0.4796]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6929, 0.5505],\n",
       "        [0.3914, 0.3372],\n",
       "        [0.9603, 0.4796]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.t(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.take(input, index) → Tensor\n",
    "- 返回一个新张量，其中包含 input 在给定索引处的元素。输入张量被视为一个 1 维张量。结果与索引具有相同的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.tensor([[4,3,5],[6,7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 5, 8])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.take(src,torch.tensor([0,2,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.take_along_dim(input, indices, dim=None, *, out=None) → Tensor\n",
    "- 选择从给定维度 dim 沿着 1 维索引从 indices 中选取 input 的值\n",
    "- 如果 dim 为 None，则输入数组被视为已展平为 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([60])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[10,30,20],[60,40,50]])\n",
    "max_idx = torch.argmax(t)\n",
    "torch.take_along_dim(t,max_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 20, 30],\n",
       "        [40, 50, 60]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_idx = torch.argsort(t,dim = 1)\n",
    "torch.take_along_dim(t,sorted_idx,dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.argmax(input) → LongTensor\n",
    "- 返回 input 张量中所有元素的最大值的索引\n",
    "- 如果有多个最大值，则返回第一个最大值的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3641, -0.5941, -0.5726, -0.0680],\n",
       "        [-0.2770,  1.1013,  0.8363, -0.6711],\n",
       "        [-0.2892,  0.0470, -0.0367, -1.8883],\n",
       "        [ 0.3052, -0.9223, -2.2735, -0.8262]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4,4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.argmax(input, dim, keepdim=False) → LongTensor\n",
    "- 返回张量沿某一维度的最大值的索引\n",
    "- keepdim（bool）- 输出张量是否保留了 dim 维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5474, -0.9632, -0.2224,  0.3720],\n",
       "        [ 1.2053,  0.3507, -0.1909, -0.6335],\n",
       "        [-0.0159, -1.9765, -0.5542,  0.6149],\n",
       "        [-0.8622, -0.7681,  0.5811, -2.1430]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4,4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 0, 3, 2])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(a,dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3],\n",
       "        [0],\n",
       "        [3],\n",
       "        [2]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(a,dim = 1,keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.argsort(input, dim=-1, descending=False, stable=False) → Tensor\n",
    "- 返回沿给定维度按值升序排序张量的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3800, 0.0587, 0.3440, 0.2428],\n",
       "        [0.1172, 0.5859, 0.0466, 0.0329],\n",
       "        [0.6791, 0.1541, 0.9897, 0.2742],\n",
       "        [0.6557, 0.6340, 0.6577, 0.1743]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(4,4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3, 2, 0],\n",
       "        [3, 2, 0, 1],\n",
       "        [1, 3, 0, 2],\n",
       "        [3, 1, 0, 2]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argsort(a,dim = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.tensor_split(input, indices_or_sections, dim=0) → List of Tensors\n",
    "- 将张量分割成多个子张量，所有子张量都是 input 的视图，根据 indices_or_sections 指定的索引或段数，沿维度 dim 进行分割\n",
    "- 如果 indices_or_sections 是一个整数 n 或一个值为 n 的零维长张量， input 将沿着维度 dim 被分割成 n 个部分。如果 input 沿着维度 dim 可以被 n 整除，每个部分将具有相同的大小，即 input.size(dim) / n 。如果 input 不能被 n 整除，前 int(input.size(dim) % n) 个部分的大小将为 int(input.size(dim) / n) + 1 ，其余部分的大小将为 int(input.size(dim) / n) \n",
    "- 如果 indices_or_sections 是一个整数列表或元组，或者是一个一维长张量，那么 input 将沿着维度 dim 在每个列表、元组或张量中的索引处进行分割。例如， indices_or_sections=[2, 3] 和 dim=0 将产生张量 input[:2] 、 input[2:3] 和 input[3:]\n",
    "- 如果 indices_or_sections 是一个张量，它必须在 CPU 上是一个零维或一维长张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(8)\n",
    "torch.tensor_split(x,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2]), tensor([3, 4]), tensor([5, 6]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(7)\n",
    "torch.tensor_split(x,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0]), tensor([1, 2, 3, 4, 5]), tensor([6]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor_split(x,(1,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6],\n",
       "        [ 7,  8,  9, 10, 11, 12, 13]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(14).reshape(2,7)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2],\n",
       "         [7, 8, 9]]),\n",
       " tensor([[ 3,  4],\n",
       "         [10, 11]]),\n",
       " tensor([[ 5,  6],\n",
       "         [12, 13]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor_split(x,3,dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [7]]),\n",
       " tensor([[ 1,  2,  3,  4,  5],\n",
       "         [ 8,  9, 10, 11, 12]]),\n",
       " tensor([[ 6],\n",
       "         [13]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor_split(x,(1,6),dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.tile(input, dims) → Tensor\n",
    "- 构建通过重复 input 的元素来构造张量的操作,dims 参数指定了每个维度中的重复次数\n",
    "- 如果 dims 指定的维度少于 input 的维度，则将 1 prepended 到 dims ，直到指定所有维度。例如，如果 input 的形状为(8, 6, 4, 2)，而 dims 为(2, 2)，则 dims 被视为(1, 1, 2, 2)\n",
    "- 类似地，如果 input 的维度少于 dims 指定的维度，则 input 被视为在零维度上未挤压，直到它具有 dims 指定的维度数。例如，如果 input 的形状为(4, 2)，而 dims 为(3, 3, 2, 2)，则 input 被视为具有形状(1, 1, 4, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 1, 2, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1,2,3])\n",
    "x.tile((2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 1, 2],\n",
       "        [3, 4, 3, 4],\n",
       "        [1, 2, 1, 2],\n",
       "        [3, 4, 3, 4]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([[1,2],[3,4]])\n",
    "torch.tile(y,(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.transpose(input, dim0, dim1) → Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7196,  0.6288,  1.3664],\n",
       "        [-1.5834,  0.9597, -0.6937]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7196, -1.5834],\n",
       "        [ 0.6288,  0.9597],\n",
       "        [ 1.3664, -0.6937]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.transpose(x,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.unbind(input, dim=0) → seq\n",
    "- 移除张量维度\n",
    "- 返回一个元组，包含给定维度上的所有切片，且已不包括该维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unbind(torch.tensor([[1,2,3],\n",
    "                           [4,5,6],\n",
    "                           [7,8,9]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.unsqueeze(input, dim) → Tensor\n",
    "- 返回一个新的张量，在指定位置插入一个大小为 1 的维度\n",
    "- 返回的张量与这个张量共享相同的基本数据\n",
    "- 可以使用范围 [-input.dim() - 1, input.dim() + 1) 内的 A dim 值。负 dim 将对应于在 dim = dim + input.dim() + 1 时应用的 unsqueeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1,2,3,4])\n",
    "torch.unsqueeze(x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(x,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.vsplit(input, indices_or_sections) → List of Tensors\n",
    "- 将 input （一个具有两个或更多维度的张量）根据 indices_or_sections 垂直分割成多个张量。每个分割都是 input 的视图\n",
    "- 这是调用 torch.tensor_split(input, indices_or_sections, dim=0)（分割维度为 0）的等效操作，但如果有 0#是一个整数，它必须能够整除分割维度，否则将抛出运行时错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(16.0).reshape(4,4)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1., 2., 3.],\n",
       "         [4., 5., 6., 7.]]),\n",
       " tensor([[ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.vsplit(t,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor([[12., 13., 14., 15.]]),\n",
       " tensor([], size=(0, 4)))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.vsplit(t,[3,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.vstack(tensors, *, out=None) → Tensor\n",
    "- 垂直（按行）堆叠张量序列\n",
    "- 这是在所有一维张量通过 torch.atleast_2d() 重塑后沿第一个轴进行连接的等效操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([4,5,6])\n",
    "torch.vstack((a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1],[2],[3]])\n",
    "b = torch.tensor([[4],[5],[6]])\n",
    "torch.vstack((a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.where(condition, input, other, *, out=None) → Tensor\n",
    "- 返回一个张量，其中包含从 input 或 other 中选择的元素，具体取决于 condition\n",
    "- 张量 condition 、 input 、 other 必须可广播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2104, -0.5108],\n",
       "        [ 0.3742,  1.8345],\n",
       "        [ 0.3634,  0.3075]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3,2)\n",
    "y = torch.ones(3,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(x > 0,1.0,0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2104, 1.0000],\n",
       "        [0.3742, 1.8345],\n",
       "        [0.3634, 0.3075]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(x > 0, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6143,  0.2004],\n",
       "        [-1.1056, -1.5155]], dtype=torch.float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2,2,dtype = torch.double)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6143, 0.2004],\n",
       "        [0.0000, 0.0000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(x > 0,x,0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.where(condition) → tuple of LongTensor\n",
    "- torch.where(condition) is identical to torch.nonzero(condition, as_tuple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.seed()      获取一个随机的随机种子\n",
    "- torch.manual_seed()    手动设置随机种子，建议设置为42\n",
    "- torch.initial_seed()    返回初始种子\n",
    "- torch.get_rng_state()       获取随机数生成器状态\n",
    "- operations on cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量的数学操作包含以下：\n",
    "- Pointwise操作     逐元素的操作，如abs, cos, sin, floor, floor_divide, pow等\n",
    "- Reduction操作     减少元素的操作，如argmax, argmin, all, any, mean, norm, var等\n",
    "- Comparsion操作    对比操作， 如ge, gt, le, lt, eq, argsort, isnan, topk\n",
    "- Spectral操作      谱操作，如短时傅里叶变换等各类信号处理的函数\n",
    "- Other操作         其它， clone， diag，flip等\n",
    "- BLAS and LAPACK操作    BLAS（Basic Linear Algebra Subprograms）基础线性代数）操作。如, addmm, dot, inner, svd等"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
